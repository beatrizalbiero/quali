\chapter{Introdução a Redes Neurais}
\label{ch:02-background}


% \epigraph{\itshape Those who cannot remember the past are condemned to compute it.''}{---Steven Pinker, \textit{Words and Rules}}

\section{Redes Neurais}

Uma rede neural é, essencialmente, um modelo de aprendizado de máquina supervisionado %[ref] 
que está a procura de aprender \textit{padrões}. Um modelo de aprendizado de máquina é uma tarefa computacional que explora algoritmos que podem aprender a partir de seus erros e fazer previsões sobre dados. A principal característica desse tipo de modelo é o caráter indutivo dos algoritmos, em oposição aos dedutivos. Esse tipo de algoritmo é normalmente inicializado com nenhuma expectativa sobre a tarefa que deve realizar e busca informações exclusivamente a partir dos dados do problema. Os possíveis tipos de aprendizado de máquina são divididos entre: \textit{supervisionados}, \textit{não-supervisionados} %ref
e por \textit{reforço}. %ref
No caso das redes neurais, o aprendizado diz-se supervisionado, pois informa-se à rede o \textit{output} esperado pelo treinamento.

A inspiração para o desenvolvimento desse tipo de modelagem surgiu a partir de estudos em neurosciência %[ref] 
que concluiram que, diante de múltiplas apresentações de um mesmo estímulo, um mesmo grupo de neurônios sofre incitação e dispara (\cite{hubel:1962}).  Analogamente, o modelo artificial é composto por uma camada de \textit{input} que recebe diferentes estímulos (representados em \ref{eq:sigmoid} por $\vect{x}$; vetores numéricos que representam o objeto a ser analisado). A informação recebida é distribuída ao longo de múltiplas conexões com a próxima camada através de uma múltiplicação com uma matriz de pesos $\vect{W}$. A matriz de pesos funciona como uma analogia às conexões existentes entre neurônios de modo que um peso maior representa uma conexão que deve ser reforçada e um peso menor representa uma conexão que deve ser reprimida. Além disso, é importante que o sistema de aprendizado não seja demasiado sensível a todo \textit{input} que receber, pois nesse caso cada \textit{input} diferente recebido alteraria completamente o modelo impossibilitando um aprendizado generalizado. Como solução, o resultado obtido a partir da multiplicação dos pesos pelos \textit{inputs} entra como argumento em uma função de ativação. A função de ativação é uma função que simula o potencial energético existente nas conexões neurais e permite que uma unidade apenas seja ativada caso o resultado dessa função atinja um \textit{threshold} mínimo. Isso permite ao sistema a produção de diferentes respostas para padrões diferentes utilizando a mesma rede e além disso, permite com que o aprendizado ocorra de forma gradual, de modo que o efeito de estímulos passados ainda perdure por um longo período mesmo após a apresentação de novos estímulos.
\input{tikz/sigmoid_plot.tex}

Uma das funções mais utilizadas na literatura é a função \textit{sigmoid}, uma função suave, diferenciável e facilmente interpretável (Fig. \ref{fig:sigmoidplot}). 

Após essa transformação, o resultado serve como novo input para a próxima camada e assim sucessivamente até a última, a camada de \textit{output}. Todas as camadas existentes entre as camadas de \textit{input} e de \textit{output} são chamadas de \textit{camadas escondidas} (Ver Fig. \ref{fig:ffd}.)

\begin{align}\label{eq:sigmoid}
p(w_{i} = 1) = \frac{1}{1+e^{\sum_{i} w_{i}x_{i}}}
\end{align}

Algebricamente, pode-se representar o processo descrito através da composição de múltiplas funções, uma vez que o resultado das operações precedentes servirão como entrada para as próximas camadas.

\begin{align}
f(\vect{x}) &= f^{(2)}(f^{(1)}(\vect{x}; \vect{W}_1); \vect{W}_2)\\
&= \sigma(\vect{W}_2 (\sigma(\vect{W}_1\vect{x})))
\end{align}

\input{tikz/ffd-generic.tex}

\subsection{Treinamento}

Para que a rede seja capaz de identificar os padrões desejados, é necessário alimentá-la com o que se espera como resposta (\textit{targets}), pois o treinamento da mesma consiste, essencialmente, na atualização das matrizes de pesos que deve ocorrer a partir da comparação entre os valores previstos pela rede (\textit{outputs}) e os \textit{targets}. A comparação entre esses valores se dá por meio de uma função de custo (\textit{Loss Function}), que representa uma forma de se quantificar o quão perto se está de uma rede ideal em que os resultados previstos correspondam exatamente aos \textit{targets}. O objetivo do aprendizado da rede é minimizar essa diferença, ou seja, encontrar o mínimo da função de custo (\cite{josh:2017}). Após a exposição a um certo número de exemplos, todos os pesos são atualizados simultaneamente com os valores que em conjunto minimizam a função de custo e portanto aproximam as previsões da rede aos \textit{targets}. %ref

\section{Modelo Apresentado em Rumelhart e McClelland (1986)}
\label{sec:arqFDD}

O esquema apresentado pelos pesquisadores Rumelhart e McClelland represetado na Fig. \ref{fig:esquemafdd} é conhecido como uma arquitetura do tipo \textit{Feed-Forward-Network (FFD)} sem camadas escondidas. Nesse caso, todos os nódulos da camada de input se conectam diretamente aos nódulos da camada de output.

Para uma melhor compreensão com relação à rede introduzida pelos autores, é necessário entender primeiramente o pré-processamento utilizado nos dados do treinamento, isto é, a análise dos verbos irregulares em unidades vetoriais simplificadas que alimentam o modelo computacional. Para tanto, é necessária a compreensão de um conceito introduzido pelos pesquisadores, o conceito de \textit{Wickelfeature}.

\subsection{Wickelfeatures}
\label{sec:wickelfeatures}
Os pesquisadores Rumelhart e McClelland propõem a caracterização de cada fonema como uma combinação simplificada de atributos em apenas 4 dimensões. O fonema \textit{d}, por exemplo, é caracterizado por um conjunto de 4 atributos: "Int.", indicando que é uma consoante interrompida; "V", indicando a vibração das cordas vocais; "Oclusi."  indicando que é uma consoante oclusiva; e "Média" por se tratar de um fonema produzido em ponto de articulação central. Seguindo este raciocínio para uma representação simplificada dos fonemas, os pesquisadores apresentam uma tabela que codifica cada fonema na língua inglesa às quatro dimensões de atributos. A Tabela \ref{tab:Tab1} foi baseada nesse sistema de codificação, porém com algumas adaptações para o português brasileiro. A tabela original apresentada pelos autores pode ser consultada no Apêndice (\ref{ch:07-appendice}. A primeira dimensão da tabela divide os fonemas em três grandes grupos: consoantes interrompidas, continuadas ou vogais. Dentre as consoantes interrompidas, encontram-se as consoantes oclusivas e nasais. Vê-se que nem todos os fonemas da tabela internacional (\ref{tab:ipa1} e \ref{tab:ipa2}) foram mapeados, apenas os mais importantes para a língua. Com relação às consoantes continuadas, estas foram subdivididas entre fricativas e líquidas (a versão dos autores ainda agrupava semi-vogais às líquidas). Ainda nesta dimensão, as vogais foram simplificadas a apenas altas ou baixas.  Com relação ao ponto de articulação (terceira dimensão), os autores agruparam bilabiais e labio-dentais em um único atributo entitulado de "anterior". Dentais e alveolares são consideradas consoantes com ponto de articulação médio. Pós-alveolares em diante são consideradas posteriores. A quarta dimensão subcategoriza as consoantes em vozeadas (V) vs. não-vozeadas (U) e as vogais em longas (L) e curtas (S). No caso da língua portuguesa não ocorre essa distinção entre as vogais, portanto essa dimensão é utilizada apenas para a dinstinção das vogais entre abertas e fechadas. No caso da vogal "u", ela representava a vogal curta associada à palavra \textit{book} (/buk/) e foi mantida nessa mesma posição.

A presença de cada um destes atributos em cada dimensão para um determinado fonema recebe o valor $1$, enquanto que a ausência, $0$. A primeira dimensão diz respeito à ordem Int - Cont - Vogal e um fonema é associado a um único atributo desta dimensão, ou seja, se o fonema for uma consoante interrompida, ele deve ser representado pelo vetor (100), se o fonema for uma consoante continuada, deve ser representado pelo vetor (010) e se for vogal, pelo vetor (001). O mesmo serve para as demais dimensões, sendo que a ordem da segunda é: Ocl - Nasal (dado que a primeira dimensão é uma consoante interrompida); Fricat - Liq (dado que a primeira dimensão é uma consoante continuada) ou Alta - Baixa caso a primeira dimensão aponte para uma vogal. Em seguida a terceira dimensão diz respeito à ordem: Anterior - Médio - Posterior. Por último, a interpretação da quarta dimensão também está condicionada à primeira dimensão, ou seja, caso o fonema seja uma consoante ele pode ser vozeado ou não-vozeado; caso seja uma vogal, longa ou curta. 
Com base na Tabela \ref{tab:Tab1}, portanto, o fonema \textit{d} passa a ser representado pelo vetor \ref{fond}. Cada sequência contida entre parênteses representa uma dimensão e os números dentro dela representam o atributo deste fonema nesta respectiva dimensão.

\begin{align}
d \Rightarrow (100)(10)(010)(10)\label{fond}
\end{align}

Com essa tabela o sistema de codificação para um único fonema está completo, porém é imprescindível que a própria sequência fonológica também seja representada já que cada fonema constitue parte de uma sequência e os padrões devem ser encontrados levando-se em consideração essa sequência. Desse modo, os pesquisadores optam por representar, em cada vetor de \textit{input}, uma sequência de três atributos (um trigrama de atributos) para cada uma das dimensões propostas (vogal - vogal - int, por exemplo). Além disso foi acrescentada uma última dimensão para representar início ou final de palavra, representado pelo símbolo "\#". Desse modo, \ref{token1}, \ref{token2} e \ref{token3} exibem as representações vetoriais do símbolo de fronteira e dos fonemas \textit{d} e \textit{a} considerando a última dimensão mencionada.

\begin{align}
\# \Rightarrow (000)(00)(000)(00)(1)\label{token1}\\
d \Rightarrow (100)(10)(010)(10)(0)\label{token2}\\
a \Rightarrow (001)(01)(010)(01)(0)\label{token3}
\end{align}

Por fim, define-se como um \textit{Wickelfeature} cada sequência de três atributos. Para alimentar a rede, os autores mapeiam cada verbo a um vetor booleano de tamanho 460, em que cada um destes valores representa a presença (ou ausência) de um \textit{Wickelfeature}. Em resumo, a rede completa é composta por duas camadas paralelas (\textit{input} e \textit{output} com 460 unidades de Wickelfeatures cada uma (Ver Fig. \ref{fig:esquemafdd}). 


\begin{table}[ht!]
\center
    \begin{tabular}{lrrrrrrr}\toprule
        &\multicolumn{2}{c}
{}&\multicolumn{1}{c}     {\textbf{Anterior}}&\multicolumn{2}{c}{\textbf{Médio}}&\multicolumn{2}{c}{\textbf{Posterior}}
        \\\cmidrule(r){3-4}\cmidrule(r){5-6}\cmidrule(r){7-8}   
        &&V/L&U/S&V/L&U/S&V/L&U/S\\\midrule
        Int.&   Oclusi. & b & p
                & d & t
                & g & k\\
                &Nasal & m
                & & n
                & & 
                & \\
        Cont. & Fricat. & v& f
                & z & s
                & j & S\\
                &Liq. &l
                & &r
                & &
                & h*\\
        Vogal & Alta & e & i 
                &   &  
                & o & u*\\
              & Baixa & a & E
              & &
              & & O
        \\\bottomrule
        Codificação: S = \ipa{S};&  j = \ipa{Z};& h\footnote{h* = Apesar de representar uma fricativa posterior, foi mantido nesta posição assim como na tabela proposta em \ref{fig:table-eng}} = x;& E = \textepsilon; & O = \textopeno & u\footnote{u* = u \& \textupsilon} & &
    \end{tabular}
    \caption{Categorização de fonemas em quatro dimensões adaptada ao Português Brasileiro}\label{tab:Tab1}
\end{table} 


\begin{center}
\scalebox{0.9}{
    \begin{tabular}{|l|cc|cc|cc|cc|cc|cc|cc|cc|cc|cc|cc|}
%\begin{tabular}{|l|cc|}
        \hline & 
            \multicolumn{2}{|c|}{\footnotesize{Bilabial}} &					% Bilabial
            \multicolumn{2}{|c|}{\footnotesize{Lab. dent.}} & 			% Labiodental
            \multicolumn{2}{|c|}{\footnotesize{Dental}} & 					% Dental
            \multicolumn{2}{|c|}{\footnotesize{Alveolar}} & 				% Alveolar
            \multicolumn{2}{|c|}{\footnotesize{P-alveo.}} & 		% Post-alveolar
            \multicolumn{2}{|c|}{\footnotesize{Retroflex}} & 				% Retroflex
            \multicolumn{2}{|c|}{\footnotesize{Palatal}} & 					% Palatal
            \multicolumn{2}{|c|}{\footnotesize{Velar}} & 					% Velar
            \multicolumn{2}{|c|}{\footnotesize{Uvular}} & 					% Uvular
            \multicolumn{2}{|c|}{\footnotesize{Pharyng.}} & 			% Pharyngeal
            \multicolumn{2}{|c|}{\footnotesize{Glottal}}  \\					% Glottal

        \hline Plosive &  							% Plosive
            p & b &													% Bilabial
            &&														% Labiodental
            \multicolumn{3}{|r}{t}&							% Dental
            \multicolumn{3}{l|}{d}&							% Alveolar
                                                                        % Post-alveolar
            \ipa{\:t} & \ipa{\:d}&									% Retroflex
            c & \textbardotlessj &														% Palatal
            k & g &													% Velar
            q & \ipa{\;G} &										% Uvular
            & \BlankCell        &								% Pharyngeal
            \ipa{P}& \BlankCell         \\								% Glottal

        \hline Nasal & 							% Nasal
            & m &													% Bilabial
            & \ipa{M} &											% Labiodental
            \multicolumn{3}{|r}{}&								% Dental
            \multicolumn{3}{l|}{n}&							% Alveolar
                                                                        % Post-alveolar
            & \ipa{\:n} &														% Retroflex
            & \textltailn &														% Palatal
            & \ipa{N} &														% Velar
            & \ipa{\;N} &														% Uvular
            \BlankCell        & \BlankCell        &		% Pharyngeal
            \BlankCell        & \BlankCell         \\		% Glottal

        \hline Trill &  								% Trill
            & \ipa{\;B}&											% Bilabial
            & &														% Labiodental
            \multicolumn{3}{|r}{}&								% Dental
            \multicolumn{3}{l|}{r}&								% Alveolar
                                                                        % Post-alveolar
            & &														% Retroflex
            & &														% Palatal
            \BlankCell        & \BlankCell        &		% Velar
            & \ipa{\;R}&											% Uvular
            & &														% Pharyngeal
            \BlankCell        & \BlankCell         \\		% Glottal

        \hline Tap/Flap &  						% Tap /Flap
            & &													% Bilabial
            & &														% Labiodental
            \multicolumn{3}{|r}{} &					% Dental
            \multicolumn{3}{l|}{\ipa{R}} &					% Alveolar
                                                                        % Post-alveolar
            & \ipa{\:r} &														% Retroflex
            & &														% Palatal
            \BlankCell        & \BlankCell        &		% Velar
            & &														% Uvular
            & &														% Pharyngeal
            \BlankCell        & \BlankCell         \\		% Glottal

        \hline Fricative & 						% Fricative
            \ipa{F} & \ipa{B} &									% Bilabial
            f & v &													% Labiodental
            \ipa{T} & \ipa{D} &									% Dental
            s & z &													% Alveolar
            \ipa{S} & \ipa{Z} &									% Post-alveolar
            \ipa{\:s} & \ipa{\:z} &								% Retroflex
            \ipa{\c{c}} & \ipa{J} &								% Palatal
            x & \ipa{G} &											% Velar
            \ipa{X} & \ipa{K} &									% Uvular
            \textcrh & \ipa{Q} &								% Pharyngeal
            h & \texthth \\										% Glottal

        \hline Lat. Fric. & 					% Lat. Fricative
            \BlankCell        & \BlankCell        &		% Bilabial
            \BlankCell        & \BlankCell        &		% Labiodental
            \multicolumn{3}{|r}{\textbeltl} &				% Dental
            \multicolumn{3}{l|}{\textlyoghlig} &			% Alveolar
                                                                        % Post-alveolar
            & &														% Retroflex
            & &														% Palatal
            & &														% Velar
            & &														% Uvular
            \BlankCell        & \BlankCell        			% Pharyngeal
            & \BlankCell        & \BlankCell         \\   % Glottal

        \hline Approx & 							% Approx.
            & &														% Bilabial
            & \ipa{V} &											% Labiodental
            \multicolumn{3}{|r}{}&								% Dental
            \multicolumn{3}{l|}{\ipa{\*r}} &					% Alveolar
                                                                        % Post-alveolar
            & \ipa{\:R} &											% Retroflex
            & j &														% Palatal
            & \textturnmrleg &									% Velar
            & &														% Uvular
            & &														% Pharyngeal
            \BlankCell        & \BlankCell         \\		% Glottal

        \hline Lat. appr. & 					% Lat. Approx
            \BlankCell        & \BlankCell        &		% Bilabial
            \BlankCell        & \BlankCell        &		% Labiodental
            \multicolumn{3}{|r}{}&								% Dental
            \multicolumn{3}{l|}{l}&								% Alveolar
                                                                        % Post-alveolar
            & \ipa{\:l} &											% Retroflex
            & \ipa{L} &												% Palatal
            & \ipa{\;L} &											% Velar
            & &														% Uvular
            \BlankCell        & \BlankCell        &		% Pharyngeal
            \BlankCell        & \BlankCell         \\		% Glottal
        \hline
    \end{tabular}
}%scalebox
\captionof{table}{Consoantes IPA}\label{tab:ipa1}
\end{center}

\begin{center}
    \begin{vowel}
        %    \putcvowel[l]{i}{1}
        \putvowel[l]{i}{0pt}{0pt}
        \putcvowel[r]{y}{1}
        \putcvowel[l]{e}{2}
        \putcvowel[r]{\o}{2}
        \putcvowel[l]{\textepsilon}{3}
        \putcvowel[r]{\oe}{3}
        \putcvowel[l]{a}{4}
        \putcvowel[r]{\textscoelig}{4}
        \putcvowel[l]{\textscripta}{5}
        \putcvowel[r]{\textturnscripta}{5}
        \putcvowel[l]{\textturnv}{6}
        \putcvowel[r]{\textopeno}{6}
        \putcvowel[l]{\textramshorns}{7}
        \putcvowel[r]{o}{7}
        \putcvowel[l]{\textturnm}{8}
        \putcvowel[r]{u}{8}
        \putcvowel[l]{\textbari}{9}
        \putcvowel[r]{\textbaru}{9}
        \putcvowel[l]{\textreve}{10}
        \putcvowel[r]{\textbaro}{10}
        \putcvowel{\textschwa}{11}
        \putcvowel[l]{\textrevepsilon}{12}
        \putcvowel[r]{\textcloserevepsilon}{12}
        \putcvowel{\textsci\ \textscy}{13}
        \putcvowel{\textupsilon}{14}
        \putcvowel{\textturna}{15}
        \putcvowel{\ae}{16}
    \end{vowel}
\captionof{table}{Vogais IPA}\label{tab:ipa2}   
\end{center} 


\subsection{Codificação}
\label{sec:cod}
Cada verbo que participa da etapa de treinamento da rede passa por um processo inicial de codificação. Tal processo é descrito a seguir utilizando o verbo \textit{dar} como exemplo.\\ 

\textbf{Entrada:} \textit{dar}\\

\textbf{Passo 1:} O token de início e final de palavra é acrescentado:\\
\hspace*{6.0em}      \#dar\#\\

\textbf{Passo 2:} O verbo é segmentado em trigramas:\\
\hspace*{6.0em}\#,d,a - d,a,r - a,r,\#\\

\textbf{Passo 3:} Um dicionário associa cada fonema a um vetor de features fonéticas, ou seja, toda possível combinação de features foi mapeada e associada a um índice de um dicionário. \\

\textbf{Passo 4:} As features dos trigramas são combinadas produzindo \textit{Wickelfeatures}.\\

%alterar a identacao aqui
Exemplos:
\begin{itemize}
\item \# - interrompida - vogal 
\item \# - oclusiva - baixa
\item \# - média - média
\item média - média - líquida
\item oclusiva - baixa - continuante
\item ...
\end{itemize}


\textbf{Passo 5:} Um dicionário de dimensão correspondente ao número total de \textit{Wickelfeatures} (460) é inicializado com $0's$ em todas as casas.\\

\textbf{Passo 6:} Cada \textit{Wickelfeature} presente no verbo analisado altera o valor do dicionário para $1$.\\

\textbf{Passo 7:} O dicionário resultante dos passos anteriores é utilizado como \textit{input} para a rede.

\subsection{Decodificação}
\label{sec:dec}

A primeira parte da decodificação da rede envolve a seleção das unidades de Wickelfeatures que compõe o início da palavra reconstituída. Primeiramente, selecionam-se todas as unidades que tem como primeiro atributo o indicador de fronteira '\#'. Em seguida, dentre estas, observam-se os Wickelfeatures com maior score. Feita esta seleção, os fonemas \textit{competem} pelas features, ou seja, todos os fonemas que compartilham de uma mesma feature observada somam pontos. Ao final deste processo, espera-se que o fonema com mais pontos seja o fonema alvo da decodificação. Decodificam-se portanto os primeiros dois fonemas.

A segunda parte do processo assemelha-se à primeira, porém ao invés da busca pelo trigrama inicial, selecionam-se todas as unidades que apresentam o atributo de fronteira na última posição. Em seguida ocorre o mesmo processo de competição para a decodificação dos dois últimos fonemas.

A terceira parte do processo de decodificação envolve uma busca de compatibilidade entre os atributos para os trigramas que não consitutem início ou final de palavra. Uma vez que o primeiro trigrama de fonemas foi decodificado, selecionam-se as próximas unidades cujas duas primeiras posições sejam compatíveis com as duas últimas do último trigrama decodificado. O processo de competição de fonemas se repete em um loop até que haja compatibilidade com o trigrama final, encontrado na segunda etapa.

O maior problema desta função de decodificação encontra-se na etapa da busca pelos trigramas que não constituem início ou fim de palavra. Dependendo da sequência, é possível que dois ou mais trigramas compartilhem de muitas unidades de Wickelfeatures, ou seja, se um Wickelfeature for ativado, não há como representar sua ativação uma segunda ou terceira vez, a sua ativação acontece apenas uma vez. Isto pode interferir na etapa de seleção de compatibilidades, fazendo com que a função não consiga sair do loop pois continua selecionando os mesmos Wickelfeatures. Esse problema já foi apontado por \cite{Pinker:1999}.
No livro, Pinker comenta a dificuldade da rede de Rumelhart e McClelland ao tentar decodificar a palavra 'algalgal' (uma palavra da língua Oykangand). Pinker faz uso desse exemplo pois, nesse caso, há inclusive a recorrência a nível de trigramas, mas o fato que é que mesmo a repetição a nível de \textit{Wickelfeatures} pode prejudicar a decodificação dos verbos. A probabilidade disso acontecer é ainda maior dependendo do comprimento dos verbos, e como apontado no Cap. \ref{ch:01-introduction}, os verbos da língua portuguesa possuem em média dois fonemas a mais que os verbos da língua inglesa. Como solução, foi necessária a implementação de uma função que considerasse apenas os trigramas fronteiriços. Desse modo foi possível verificar a capacidade da rede em capturar os processos de flexão irregulares que normalmente ocorrem no início ou no final das palavras. Para a palavra "postar", por exemplo, a irregularidade ocorre ao transmutar a vogal arredondada posterior semi-fechada para semi-aberta. Como essa irregularidade já pode ser detectada na primeira etapa da decodificação, pode-se avaliar o desempenho da rede baseado nesta etapa e desconsiderar a decodificação dos trigramas intermediários 'Ost' e 'stu' sem perda de informação.  

Para concluir, a implementação de uma rede FFD para a predição de irregularidades verbais do português brasileiro mostrou-se uma tarefa de grande dificuldade e motivou a construção de novos tipos de arquitetura para o alcance dos objetivos propostos. 


\section{Redes Neurais Recorrentes}
\label{sec:RNN}

A Seção \ref{sec:wickelfeatures} serve de exemplo para problematizar o uso da arquitetura FFD na modelagem dos verbos do português brasileiro. Certamente, os processos descritos nas subseções \ref{sec:cod} e \ref{sec:dec} não se apresentam como componentes simples ou orgânicos, tão pouco eficientes. O problema do processamento de dados sequenciais motivou o desenvolvimento de uma nova família de redes neurais, as Redes Neurais Recorrentes (RNN). 

A ideia central desse modelo consiste na retroalimentação dos elementos sequenciais, de modo que o \textit{input} de cada um deles serve, não somente para a previsão do próximo item da sequência, mas também para a formação de um componente intermediário, um \textit{estado}. Esses estados, representados na Figura \ref{fig:unfoldedrnn} como $\vect{h}$'s funcionam como uma espécie de memória condensada dos elementos precedentes e influenciam diretamente os estados posteriores. Essa é uma maneira de retransmitir a cada momento os efeitos dos inputs anteriores para o restante da sequência (\cite{Goodfellow-et-al-2016}).

%\input{tikz/rnn-cell.tex}
\input{tikz/rnn-unrolled.tex}

Os estados indicados na Fig. \ref{fig:unfoldedrnn} são calculados a partir da equação recorrente\footnote{O estado $\vect{h(0)}$ é normalmente inicializado de maneira aleatória.}:

\begin{equation}
\vect{h}^{(t)} = g(\vect{h}^{(t-1)}, \vect{x}^{(t)}; \vect{\theta})
\label{eq:rnn}
\end{equation}

\subsection{Treinamento}

Na prática, essa arquitetura simplificaria algumas das etapas do processo de predição de irregularidades verbais. A primeira delas seria o processo de codificação dos verbos. Por exemplo, ao se utilizar uma rede recorrente, não há mais a necessidade da ativação de unidades em trigramas. Basta analisar item a item da sequência. No treinamento desse tipo de rede, os inputs são dados por cada um dos itens da série e, os targets, por sua vez, são seus respectivos itens subsequentes. Tokens de final de sequência (<eos>) são acrescentados nos targets para que a rede considere como possibilidade o fim da sequência. O processo de decodificação dos verbos também torna-se desnecessário uma vez que o output é computado item após item. A Fig. \ref{fig:rnnpractice} ilustra o problema da predição de fonemas dada uma base de verbos conjugados. No exemplo, considera-se o treinamento da palavra \textit{"pa.r\textupsilon"}. 

Apesar da praticidade desse tipo de rede, ela infelizmente apresenta alguns problemas para tarefas que envolvem dependências de longa distância, o que normalmente é o caso quando se trata de tarefas linguísticas. O problema ocorre porque, a cada \textit{input}, as matrizes de pesos são atualizadas fazendo com que as últimas informações recebidas sejam mais relevantes do que as anteriores, impedindo o progresso do treinamento. Dessa forma, apresentam-se na literatura algumas soluções. Entre elas, uma arquitetura conhecida como \textit{Long Short-Term Memory - LSTM}.

\input{tikz/rnn-practice.tex}

\subsection{Long Short-Term Memory}
\label{sec:LSTM}

Uma LSTM continua sendo uma RNN, porém existem algumas especificidades em sua arquitetura que a tornam mais elaborada e mais adequada para atender aos treinamentos que envolvem longa dependência. A chave para entender como as LSTM's lidam com esse problema está no entendimento de um componente chamado de \textit{cell state}. Esse componente funciona como uma corrente transportadora de informação e é regulado por estruturas que funcionam como válvulas (os \textit{gates}). Os \textit{gates} são uma forma de filtragem de informação. Eles são basicamente responsáveis por selecionar informações antigas que podem ser esquecidas e novas que sejam relevantes. 

%imagem de LSTM
\input{tikz/lstm.tex}

A Fig. \ref{fig:lstm} deve ser interpretada da seguinte forma: A cada novo input ($x(t)$), uma camada LSTM recebe o estado $h(t-1)$ gerado pelo input anterior assim como uma RNN comum receberia, mas recebe também o Cell State de (t-1). O Cell State traz consigo uma lista de todas as informações que ele considera que devem ser carregadas e mantidas durante todo o treinamento. Após a intervenção dos gates, o modelo pode decidir por agregar novas informações e esquecer outras nessa lista, gerando assim um novo Cell State (t). Além disso, a passagem desse input por essa camada também gera um novo estado ($h(t)$) que, além de servir como referência para o cálculo do output de (t), servirá também como referência para o próximo processamento ($x(t+1)$). (\cite{Goodfellow-et-al-2016})

\section{Encoder-Decoder}
\label{sec:enc-dec}

Como apresentado na Seção \ref{sec:RNN}, os modelos de LSTM mostram-se úteis na tarefa de predição de dados sequenciais. No entanto, o problema apresentado por esta pesquisa vai além disto. O objetivo desta pesquisa visa encontrar modelos preditivos de irregularidades verbais do português brasileiro tendo como referência as suas respectivas formas no infinitivo, assim como foi apresentado pelos pesquisadores Rumelhart e McClelland para a língua inglesa. Nesse sentido, treinar uma LSTM em uma base de verbos já conjugados não faz sentido. É preciso considerar um modelo que seja capaz de traduzir os verbos de uma forma para a outra. \\

Um modelo de mapeamento Encoder-Decoder é um sistema composto por duas RNNs cuja principal função é mapear sequências. A primeira rede, chamada encoder, mapeia os dados de uma determinada sequência de origem e os transforma em uma representação codificada, esta alimenta então a segunda rede, decoder, que, por sua vez, tem como objetivo gerar uma nova sequência alvo.  %ref
Modelos do tipo Encoder-Decoder (também conhecidos como \textit{Seq2Seq}) tem sido bastante utilizados em tarefas linguísticas, especialmente no desenvolvimento de sistemas de diálogo e em tradução automática. Em um contexto de tradução do inglês para o português, por exemplo, uma rede encoder seria treinada com um corpus em inglês, o qual apresenta determinadas probabilidades de arranjo sintático e valores semânticos, ou seja, uma gestalt sequencial armazenada em uma matriz. A rede decoder, ao ser treinada na língua alvo, isto é, português, implicitamente leva em consideração tanto as características sintático-semânticas desta quanto a gestalt da rede encoder, gerando desta forma uma nova sequência que contempla ambas as línguas e assemelha-se a uma tradução humana. Tecnicamente, a gestalt gerada pela rede encoder nada mais é do que o último estado consolidado alcançado a partir dos treinamentos dessa rede. A decoder, portanto, recebe como input, não só os exemplos da língua alvo como também o estado gerado pela rede encoder. Por fim, obtem-se um sistema capaz de predizer os resultados de uma sequência alvo a partir de uma sequência de origem.

Em função de uma similaridade conceitual, este modelo foi escolhido, uma vez que possibilita uma interação mediada pela essência de duas qualidades de padrões sequencias e tem como objetivo a produção de um resultado orgânico, semelhante, por exemplo, a intuição de um falante nativo que advinha a conjugação da primeira pessoa do presente a partir de um verbo no infinitivo. 
\input{tikz/seq2seq.tex}




